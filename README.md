# ğŸ“Œ Setting Up Kibana Dashboard

1. Start Elasticsearch & Kibana

```bash
sudo systemctl start elasticsearch
sudo systemctl start kibana
```

2. Index Pattern in Kibana

* Open Kibana (http://localhost:5601)
* Go to Stack Management â†’ Index Patterns
* Create index pattern: smpp_submit_sm_live
* Add fields: source_addr, destination_addr, short_message, sequence_number, etc.

3. Create Dashboard

* Go to Dashboard â†’ Create New Dashboard
* Add Visualizations:
    - Bar Chart: Count of SMPP messages by source
    - Line Chart: SMPP message rate over time
    - Table: Raw Submit_SM message logs



# ğŸš€ How to Run?

## Option 1: Live Capture Mode

```bash
python main.py --mode live
```

# Option 2: Read from PCAP File

```bash
python main.py --mode pcap --file path/to/your.pcap
```


# ğŸ”· Step-by-Step Kafka Integration

## ğŸ›  Step 1: Install Kafka & Dependencies
First, install the Kafka Python client:

```bash
pip install kafka-python
```

Ensure Kafka & Zookeeper are running:

```bash
bin/zookeeper-server-start.sh config/zookeeper.properties
bin/kafka-server-start.sh config/server.properties
```

Create topics:

```bash
bin/kafka-topics.sh --create --topic smpp-submit --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
bin/kafka-topics.sh --create --topic smpp-deliver --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
bin/kafka-topics.sh --create --topic smpp-response --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

ğŸ”· Step 1: Kafka & Spark Streaming for SMPP Analytics
ğŸ“Œ Architecture Overview
1ï¸âƒ£ Kafka Producers (from packet capture) send data to Kafka.
2ï¸âƒ£ Spark Streaming consumes Kafka topics and processes SMPP analytics.
3ï¸âƒ£ Processed data is sent to Elasticsearch for storage & visualization.
4ï¸âƒ£ Kafka Streams detects high-latency Submit/Submit-Resp pairs.

ğŸ”· Step 2: Kafka Streams for Latency Alerting
If Submit-Resp latency exceeds a threshold, send an alert.
âœ… Continuously monitors latency & triggers alerts.
âœ… Integrate with Slack, Email, or Webhooks for alerting.

ğŸ”· Step 3: Grafana + Elasticsearch for Real-Time Dashboards
1. Install Grafana + Elasticsearch

2. Configure Grafana Datasource

- Go to Grafana â†’ Configuration â†’ Data Sources
- Add Elasticsearch (http://elasticsearch:9200)

3. Create Dashboard with Panels
- Latency Panel: avg(latency) by sequence_number
- Error Panel: count(command_status != 0)

âœ… Grafana provides a live view of SMPP performance.

2ï¸âƒ£ Test Endpoints
Get latency:
```bash
curl http://localhost:8000/latency/12345
```

Get high latency (above 500ms):
```bash
curl http://localhost:8000/high-latency?min_latency=500
```

Get errors:
```bash
curl http://localhost:8000/errors
```

Get raw packet:
```bash
curl http://localhost:8000/packet/1
```

```graphql
smpp_monitoring/
â”‚â”€â”€ ğŸ“‚ config/            # Configuration files (logging, database, etc.)
â”‚   â”‚â”€â”€ config.py         # Environment variables, PostgreSQL & Kafka settings
â”‚   â”‚â”€â”€ logging_config.py # Logging setup
â”‚
â”‚â”€â”€ ğŸ“‚ database/          # PostgreSQL database interactions
â”‚   â”‚â”€â”€ db.py             # Database connection & setup
â”‚   â”‚â”€â”€ queries.py        # SQL queries for inserting & fetching data
â”‚
â”‚â”€â”€ ğŸ“‚ smpp/              # SMPP packet processing
â”‚   â”‚â”€â”€ smpp_processor.py # Extract and analyze SMPP packets
â”‚   â”‚â”€â”€ packet_parser.py  # Parse Submit_SM, Submit_SM_RESP, Deliver_SM, etc.
â”‚   â”‚â”€â”€ pcap_reader.py    # Read from PCAP files
â”‚   â”‚â”€â”€ packet_sniffer.py # Live network capture
â”‚
â”‚â”€â”€ ğŸ“‚ kafka/             # Kafka Integration
â”‚   â”‚â”€â”€ producer.py       # Push SMPP packet data to Kafka
â”‚   â”‚â”€â”€ consumer.py       # Process Kafka messages in Spark
â”‚   â”‚â”€â”€ kafka_config.py   # Kafka settings
â”‚
â”‚â”€â”€ ğŸ“‚ analytics/         # Spark analytics for latency detection
â”‚   â”‚â”€â”€ spark_processor.py  # Spark job for real-time analytics
â”‚   â”‚â”€â”€ alerting.py       # Kafka Streams for high-latency alerts
â”‚
â”‚â”€â”€ ğŸ“‚ api/               # REST API for querying SMPP insights
â”‚   â”‚â”€â”€ api.py            # FastAPI for serving insights
â”‚   â”‚â”€â”€ models.py         # Data models (Pydantic)
â”‚   â”‚â”€â”€ routes/           # API endpoints
â”‚       â”‚â”€â”€ smpp_routes.py  # SMPP-specific routes
â”‚       â”‚â”€â”€ healthcheck.py  # Health check endpoints
â”‚
â”‚â”€â”€ ğŸ“‚ dashboards/        # Visualization with Grafana
â”‚   â”‚â”€â”€ grafana_config/   # Grafana dashboards & queries
â”‚
â”‚â”€â”€ ğŸ“‚ tests/             # Unit & integration tests
â”‚   â”‚â”€â”€ test_db.py        # Test database queries
â”‚   â”‚â”€â”€ test_parser.py    # Test SMPP parsing
â”‚   â”‚â”€â”€ test_api.py       # Test API responses
â”‚
â”‚â”€â”€ Dockerfile            # Docker configuration
â”‚â”€â”€ docker-compose.yml    # Multi-service container orchestration
â”‚â”€â”€ requirements.txt      # Python dependencies
â”‚â”€â”€ README.md             # Project documentation
```